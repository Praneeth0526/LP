{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-18T14:58:37.739239Z",
     "start_time": "2025-08-18T14:58:36.834965Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def export_excel_sheets_to_csv(excel_path, output_dir):\n",
    "    \"\"\"\n",
    "    Reads an Excel file and saves each sheet as a separate CSV file.\n",
    "\n",
    "    Args:\n",
    "        excel_path (str): The path to the input Excel file.\n",
    "        output_dir (str): The directory where CSV files will be saved.\n",
    "    \"\"\"\n",
    "    # --- 1. Ensure the output directory exists ---\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Output directory '{output_dir}' is ready.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: Could not create directory {output_dir}. Reason: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Check if the source Excel file exists ---\n",
    "    if not os.path.exists(excel_path):\n",
    "        print(f\"❌ Error: Input file not found at '{excel_path}'\")\n",
    "        print(\"Please ensure you have run the data consolidation step first.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Load the Excel file and process each sheet ---\n",
    "    try:\n",
    "        xls = pd.ExcelFile(excel_path)\n",
    "        sheet_names = xls.sheet_names\n",
    "        print(f\"Found sheets to export: {sheet_names}\")\n",
    "\n",
    "        for sheet_name in sheet_names:\n",
    "            print(f\"Processing sheet: '{sheet_name}'...\")\n",
    "\n",
    "            # Read the sheet into a pandas DataFrame\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "\n",
    "            # Define the output path for the new CSV file\n",
    "            csv_filepath = os.path.join(output_dir, f\"{sheet_name}.csv\")\n",
    "\n",
    "            # Save the DataFrame to CSV, without the pandas index\n",
    "            df.to_csv(csv_filepath, index=False)\n",
    "            print(f\"✅ Successfully exported '{sheet_name}' to '{csv_filepath}'\")\n",
    "\n",
    "        print(\"\\nAll sheets have been exported successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- Define file paths ---\n",
    "input_excel_file = 'Data/combined_report.xlsx'\n",
    "output_csv_directory = 'Data/csv_sheets'\n",
    "\n",
    "# --- Run the export function ---\n",
    "export_excel_sheets_to_csv(input_excel_file, output_csv_directory)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory 'Data/csv_sheets' is ready.\n",
      "Found sheets to export: ['DMA', 'original', 'Mapping', 'Job_Roles_And_Skills']\n",
      "Processing sheet: 'DMA'...\n",
      "✅ Successfully exported 'DMA' to 'Data/csv_sheets/DMA.csv'\n",
      "Processing sheet: 'original'...\n",
      "✅ Successfully exported 'original' to 'Data/csv_sheets/original.csv'\n",
      "Processing sheet: 'Mapping'...\n",
      "✅ Successfully exported 'Mapping' to 'Data/csv_sheets/Mapping.csv'\n",
      "Processing sheet: 'Job_Roles_And_Skills'...\n",
      "✅ Successfully exported 'Job_Roles_And_Skills' to 'Data/csv_sheets/Job_Roles_And_Skills.csv'\n",
      "\n",
      "All sheets have been exported successfully!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:07:18.074860Z",
     "start_time": "2025-08-14T11:07:17.961574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Load and preprocess the dataset\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "def extract_skills_features(df):\n",
    "    \"\"\"Extract and encode skills from the dataset\"\"\"\n",
    "    df['Technical Skill'] = df['Technical Skill'].astype(str)\n",
    "    df['Programming Languages'] = df['Programming Languages'].astype(str)\n",
    "    df['Soft Skills'] = df['Soft Skills'].astype(str)\n",
    "\n",
    "    # Technical Skills\n",
    "    all_tech_skills = set(skill.strip() for skills in df['Technical Skill'].dropna() for skill in skills.split('/'))\n",
    "    tech_features = []\n",
    "    for skill in sorted(all_tech_skills):\n",
    "        if not skill: continue\n",
    "        feature_name = f'Tech_{skill.replace(\" \", \"_\").replace(\"/\", \"_\")}'\n",
    "        df[feature_name] = df['Technical Skill'].str.contains(skill, case=False, na=False).astype(int)\n",
    "        tech_features.append(feature_name)\n",
    "\n",
    "    # Programming Languages\n",
    "    all_prog_langs = set(lang.strip() for langs in df['Programming Languages'].dropna() for lang in langs.split('/'))\n",
    "    prog_features = []\n",
    "    for lang in sorted(all_prog_langs):\n",
    "        if not lang: continue\n",
    "        feature_name = f'Prog_{lang.replace(\" \", \"_\").replace(\"/\", \"_\")}'\n",
    "        df[feature_name] = df['Programming Languages'].str.contains(lang, case=False, na=False).astype(int)\n",
    "        prog_features.append(feature_name)\n",
    "\n",
    "    # Soft Skills\n",
    "    all_soft_skills = set(skill.strip() for skills in df['Soft Skills'].dropna() for skill in skills.split('/'))\n",
    "    soft_features = []\n",
    "    for skill in sorted(all_soft_skills):\n",
    "        if not skill: continue\n",
    "        feature_name = f'Soft_{skill.replace(\" \", \"_\").replace(\"/\", \"_\")}'\n",
    "        df[feature_name] = df['Soft Skills'].str.contains(skill, case=False, na=False).astype(int)\n",
    "        soft_features.append(feature_name)\n",
    "\n",
    "    return df, tech_features, prog_features, soft_features\n",
    "\n",
    "def create_intelligence_features(df):\n",
    "    \"\"\"Create features from intelligence scores\"\"\"\n",
    "    intelligence_features = []\n",
    "    intel_cols = ['Linguistic', 'Musical', 'Bodily', 'Logical - Mathematical',\n",
    "                  'Spatial-Visualization', 'Interpersonal', 'Intrapersonal', 'Naturalist']\n",
    "\n",
    "    for col in intel_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(df[col].median())\n",
    "            intelligence_features.append(col)\n",
    "\n",
    "    if intelligence_features:\n",
    "        df['Intelligence_Total'] = df[intelligence_features].sum(axis=1)\n",
    "        df['Intelligence_Average'] = df[intelligence_features].mean(axis=1)\n",
    "        intelligence_features.extend(['Intelligence_Total', 'Intelligence_Average'])\n",
    "\n",
    "    return df, intelligence_features\n",
    "\n",
    "def create_additional_features(df):\n",
    "    \"\"\"Create additional features from the dataset\"\"\"\n",
    "    additional_features = []\n",
    "    df['Year_Numeric'] = pd.to_numeric(df['Year'], errors='coerce').fillna(0)\n",
    "    additional_features.append('Year_Numeric')\n",
    "\n",
    "    df['Technical_Rating'] = pd.to_numeric(df['Rating'], errors='coerce').fillna(0)\n",
    "    df['Soft_Rating'] = pd.to_numeric(df['Rating.1'], errors='coerce').fillna(0)\n",
    "    additional_features.extend(['Technical_Rating', 'Soft_Rating'])\n",
    "\n",
    "    df['Has_Projects'] = df['Projects'].map({'Yes': 1, 'No': 0}).fillna(0)\n",
    "    additional_features.append('Has_Projects')\n",
    "\n",
    "    if 'Score' in df.columns:\n",
    "        df['Overall_Score'] = pd.to_numeric(df['Score'], errors='coerce').fillna(0)\n",
    "        additional_features.append('Overall_Score')\n",
    "\n",
    "    df['Technical_Competency'] = (df['Technical_Rating'] * 0.6 + df['Soft_Rating'] * 0.4)\n",
    "    additional_features.append('Technical_Competency')\n",
    "\n",
    "    tech_count = df['Technical Skill'].fillna('').str.split('/').str.len()\n",
    "    prog_count = df['Programming Languages'].fillna('').str.split('/').str.len()\n",
    "    df['Skill_Diversity'] = (tech_count + prog_count).fillna(0)\n",
    "    additional_features.append('Skill_Diversity')\n",
    "\n",
    "    return df, additional_features\n",
    "\n",
    "def prepare_target_variable(df):\n",
    "    \"\"\"Prepare target variable for job recommendation\"\"\"\n",
    "    le_job = LabelEncoder()\n",
    "    df['Job_Target'] = le_job.fit_transform(df['Recommended Job'].fillna('Unknown'))\n",
    "    print(f\"Number of unique jobs: {len(le_job.classes_)}\")\n",
    "    return df, le_job\n",
    "\n",
    "def train_model(df, features, target_col, model_name=\"Model\"):\n",
    "    \"\"\"A generic function to train and evaluate a model.\"\"\"\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    X = df[features].fillna(0)\n",
    "    y = df[target_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=150, random_state=42, class_weight='balanced', n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(f\"{model_name} - Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Features used: {len(features)}\")\n",
    "    return model\n",
    "\n",
    "def save_artifacts(skills_model, holistic_model, skills_features, all_features, le_job):\n",
    "    \"\"\"Saves all the necessary models and objects to the 'models/' directory.\"\"\"\n",
    "    print(\"\\n=== Saving Models and Artifacts ===\")\n",
    "    model_dir = 'models'\n",
    "\n",
    "    # Save models\n",
    "    joblib.dump(skills_model, os.path.join(model_dir, 'skills_model.pkl'))\n",
    "    joblib.dump(holistic_model, os.path.join(model_dir, 'holistic_model.pkl'))\n",
    "\n",
    "    # Save feature lists\n",
    "    joblib.dump(skills_features, os.path.join(model_dir, 'skills_features.pkl'))\n",
    "    joblib.dump(all_features, os.path.join(model_dir, 'holistic_features.pkl'))\n",
    "\n",
    "    # Save the Label Encoder\n",
    "    joblib.dump(le_job, os.path.join(model_dir, 'career_label_encoder.pkl'))\n",
    "\n",
    "    print(f\"✅ All models and artifacts saved successfully to the '{model_dir}/' directory!\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    print(\"🚀 Job Recommendation Model Training Pipeline 🚀\")\n",
    "\n",
    "    # 1. Load data\n",
    "    print(\"\\n📥 Loading dataset...\")\n",
    "    df = load_and_preprocess_data('Data/csv_sheets/DMA.csv') # Using the newly created CSV\n",
    "\n",
    "    # 2. Feature Engineering\n",
    "    print(\"\\n🔧 Engineering features...\")\n",
    "    df, tech_features, prog_features, soft_features = extract_skills_features(df)\n",
    "    df, intelligence_features = create_intelligence_features(df)\n",
    "    df, additional_features = create_additional_features(df)\n",
    "\n",
    "    # Define feature sets\n",
    "    skills_features = tech_features + prog_features + soft_features + ['Technical_Competency', 'Skill_Diversity', 'Has_Projects']\n",
    "    all_features = skills_features + intelligence_features + additional_features\n",
    "\n",
    "    # Remove duplicates from feature lists\n",
    "    skills_features = sorted(list(set(skills_features)))\n",
    "    all_features = sorted(list(set(all_features)))\n",
    "\n",
    "    # 3. Prepare target variable\n",
    "    print(\"\\n🎯 Preparing target variable...\")\n",
    "    df, le_job = prepare_target_variable(df)\n",
    "\n",
    "    # 4. [FIX] Identify and remove classes with only one sample\n",
    "    print(\"\\n🔍 Checking for rare job categories...\")\n",
    "    class_counts = df['Job_Target'].value_counts()\n",
    "    rare_classes = class_counts[class_counts < 2].index\n",
    "\n",
    "    if not rare_classes.empty:\n",
    "        rare_job_names = le_job.inverse_transform(rare_classes)\n",
    "        print(f\"⚠️ Found rare job categories with only 1 sample: {list(rare_job_names)}\")\n",
    "\n",
    "        original_rows = len(df)\n",
    "        df = df[~df['Job_Target'].isin(rare_classes)]\n",
    "        print(f\"Removed {original_rows - len(df)} rows belonging to these categories.\")\n",
    "        print(f\"New dataset shape for training: {df.shape}\")\n",
    "    else:\n",
    "        print(\"✅ No rare categories found. All classes have enough samples for splitting.\")\n",
    "\n",
    "    # 5. Train models\n",
    "    skills_model = train_model(df, skills_features, 'Job_Target', \"Skills-Based Model\")\n",
    "    holistic_model = train_model(df, all_features, 'Job_Target', \"Holistic Model\")\n",
    "\n",
    "    # 6. Save all artifacts\n",
    "    save_artifacts(skills_model, holistic_model, skills_features, all_features, le_job)\n",
    "\n",
    "    print(\"\\n🎉 Training pipeline completed successfully! 🎉\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "51bcd50035ae6a46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Job Recommendation Model Training Pipeline 🚀\n",
      "\n",
      "📥 Loading dataset...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/csv_sheets/DMA.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 200\u001B[39m\n\u001B[32m    198\u001B[39m \u001B[38;5;66;03m# Run the main function\u001B[39;00m\n\u001B[32m    199\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m200\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 153\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    151\u001B[39m \u001B[38;5;66;03m# 1. Load data\u001B[39;00m\n\u001B[32m    152\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m📥 Loading dataset...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m df = \u001B[43mload_and_preprocess_data\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mData/csv_sheets/DMA.csv\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# Using the newly created CSV\u001B[39;00m\n\u001B[32m    155\u001B[39m \u001B[38;5;66;03m# 2. Feature Engineering\u001B[39;00m\n\u001B[32m    156\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m🔧 Engineering features...\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mload_and_preprocess_data\u001B[39m\u001B[34m(file_path)\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload_and_preprocess_data\u001B[39m(file_path):\n\u001B[32m     16\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Load and preprocess the dataset\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     18\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDataset shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdf.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     19\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mColumns: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdf.columns.tolist()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/Lets Pro/Skill Gap/myenv1/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/Lets Pro/Skill Gap/myenv1/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/Lets Pro/Skill Gap/myenv1/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/Lets Pro/Skill Gap/myenv1/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1887\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1889\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/Lets Pro/Skill Gap/myenv1/lib/python3.12/site-packages/pandas/io/common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'Data/csv_sheets/DMA.csv'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4d6d85051751c399"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
